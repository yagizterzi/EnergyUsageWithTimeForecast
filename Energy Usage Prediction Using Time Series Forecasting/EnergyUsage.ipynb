{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEho4UdxJgxN"
      },
      "source": [
        "# Import Required Libraries\n",
        "Import the necessary libraries, including pandas, numpy, matplotlib, plotly, sklearn, xgboost, seaborn, multiprocessing, and logging."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8xrOgNfIgHg"
      },
      "outputs": [],
      "source": [
        "from ucimlrepo import fetch_ucirepo\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from xgboost import XGBRegressor\n",
        "import seaborn as sns\n",
        "from multiprocessing import Pool\n",
        "import logging\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mRdCK27Jo3i"
      },
      "source": [
        "# Configure Logging\n",
        "Configure the logging settings to display information with timestamps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Dt6UAnhJskf"
      },
      "outputs": [],
      "source": [
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvigow_QJu5M"
      },
      "source": [
        "# Define fit_model Function\n",
        "Define the fit_model function that trains an XGBoost model on a chunk of the dataset.Splitting data into chunks improves memory efficiency and processing speed, especially for large datasets that cannot fit into RAM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NsjxivPyJ7te"
      },
      "outputs": [],
      "source": [
        "# Define fit_model Function\n",
        "def fit_model(df_chunk):\n",
        "    \"\"\"\n",
        "    Trains an XGBoost model on a chunk of the dataset.\n",
        "\n",
        "    Parameters:\n",
        "    df_chunk (DataFrame): A chunk of the dataset containing features and target variable.\n",
        "\n",
        "    Returns:\n",
        "    XGBRegressor: Trained XGBoost model.\n",
        "    \"\"\"\n",
        "    # Separate features and target variable\n",
        "    X = df_chunk.drop(columns=[\"y\", \"ds\"])\n",
        "    y = df_chunk[\"y\"]\n",
        "\n",
        "    # Filter NaN, infinite, or extremely large values\n",
        "    valid_indices = y.notna() & np.isfinite(y) & (y < np.finfo(np.float64).max)\n",
        "    X = X.loc[valid_indices]\n",
        "    y = y.loc[valid_indices]\n",
        "\n",
        "    # Initialize and train the XGBoost model\n",
        "    model = XGBRegressor()\n",
        "    model.fit(X, y)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcXQyI4oKLrM"
      },
      "source": [
        "# Define main Function\n",
        "Define the main function that orchestrates the data loading, preprocessing, model training, and evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFjVH5OOKQOz",
        "outputId": "d50f9b90-b2e7-45ad-e37a-418f9de80b69"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/ucimlrepo/fetch.py:97: DtypeWarning:\n",
            "\n",
            "Columns (2,3,4,5,6,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "\n",
            "<ipython-input-17-e543767a4470>:25: FutureWarning:\n",
            "\n",
            "DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "\n",
            "<ipython-input-17-e543767a4470>:26: FutureWarning:\n",
            "\n",
            "DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "\n",
            "/usr/local/lib/python3.11/dist-packages/numpy/core/fromnumeric.py:59: FutureWarning:\n",
            "\n",
            "'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Define main Function\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Orchestrates the data loading, preprocessing, model training, and evaluation.\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "    logging.info(\"Starting the main function\")\n",
        "\n",
        "    # Load dataset\n",
        "    dataset = fetch_ucirepo(id=235)\n",
        "    X = dataset.data.features\n",
        "    y = dataset.data.targets\n",
        "\n",
        "    # Combine data\n",
        "    df = pd.concat([X, y], axis=1)\n",
        "\n",
        "    # Create date column\n",
        "    df[\"date\"] = pd.to_datetime(df[\"Date\"] + \" \" + df[\"Time\"], dayfirst=True)\n",
        "\n",
        "    # Drop unnecessary columns\n",
        "    df.drop(columns=[\"Date\", \"Time\"], inplace=True)\n",
        "    df.set_index(\"date\", inplace=True)\n",
        "\n",
        "    # Fill missing values\n",
        "    df.fillna(method=\"ffill\", inplace=True)\n",
        "    df.fillna(method=\"bfill\", inplace=True)\n",
        "\n",
        "    # Convert 'Global_active_power' to numeric\n",
        "    df[\"Global_active_power\"] = pd.to_numeric(df[\"Global_active_power\"], errors=\"coerce\")\n",
        "\n",
        "    # Prepare dataset for model\n",
        "    df_xgboost = df[[\"Global_active_power\"]].reset_index()\n",
        "    df_xgboost.columns = [\"ds\", \"y\"]\n",
        "\n",
        "    # Add date features\n",
        "    df_xgboost[\"hour\"] = df_xgboost[\"ds\"].dt.hour\n",
        "    df_xgboost[\"day\"] = df_xgboost[\"ds\"].dt.day\n",
        "    df_xgboost[\"month\"] = df_xgboost[\"ds\"].dt.month\n",
        "    df_xgboost[\"year\"] = df_xgboost[\"ds\"].dt.year\n",
        "\n",
        "    # Clean NaN and infinite values\n",
        "    df_xgboost.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "    df_xgboost.dropna(inplace=True)\n",
        "\n",
        "    # Split into train and test sets\n",
        "    train_size = int(len(df_xgboost) * 0.8)\n",
        "    train_df = df_xgboost[:train_size]\n",
        "    test_df = df_xgboost[train_size:]\n",
        "\n",
        "    # Split train set into chunks\n",
        "    num_chunks = 4\n",
        "    train_chunks = np.array_split(train_df, num_chunks)\n",
        "\n",
        "    # Train model in parallel\n",
        "    logging.info(\"Starting model training with multiprocessing\")\n",
        "    with Pool(num_chunks) as pool:\n",
        "        models = pool.map(fit_model, train_chunks)\n",
        "\n",
        "    # Make predictions\n",
        "    logging.info(\"Making predictions\")\n",
        "    forecasts = [model.predict(test_df.drop(columns=[\"ds\", \"y\"])) for model in models]\n",
        "    forecast = pd.DataFrame({\"ds\": test_df[\"ds\"], \"yhat\": np.mean(forecasts, axis=0)})\n",
        "\n",
        "    # Align actual and forecasted values\n",
        "    common_dates = test_df.set_index(\"ds\").index.intersection(forecast.set_index(\"ds\").index)\n",
        "    if len(common_dates) == 0:\n",
        "        logging.error(\"No common dates found between actual and forecasted values. Check the data.\")\n",
        "        return\n",
        "\n",
        "    y_true = test_df.set_index(\"ds\").loc[common_dates, \"y\"]\n",
        "    y_pred = forecast.set_index(\"ds\").loc[common_dates, \"yhat\"]\n",
        "\n",
        "    logging.info(f\"Before align - y_true: {len(y_true)}, y_pred: {len(y_pred)}\")\n",
        "\n",
        "    y_true, y_pred = y_true.align(y_pred, join='inner')\n",
        "    y_true.dropna(inplace=True)\n",
        "    y_pred.dropna(inplace=True)\n",
        "\n",
        "    if len(y_true) != len(y_pred):\n",
        "        y_pred = y_pred.reindex(y_true.index, method='nearest')\n",
        "\n",
        "    logging.info(f\"After align - y_true: {len(y_true)}, y_pred: {len(y_pred)}\")\n",
        "\n",
        "    if len(y_true) != len(y_pred):\n",
        "        logging.error(f\"Final y_true length: {len(y_true)}, y_pred length: {len(y_pred)}\")\n",
        "        return\n",
        "\n",
        "    # Visualize the forecast\n",
        "    fig = px.line(forecast, x=\"ds\", y=\"yhat\", title=\"Electricity Consumption Forecast\")\n",
        "    fig.show()\n",
        "\n",
        "    # Plot: Actual vs Predicted\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(y_true, label=\"Actual Values\", color=\"blue\")\n",
        "    plt.plot(y_pred, label=\"Predicted Values\", color=\"red\", linestyle=\"dashed\")\n",
        "    plt.xlabel(\"Date\")\n",
        "    plt.ylabel(\"Energy Consumption\")\n",
        "    plt.title(\"Actual vs. Predicted Energy Consumption\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Residuals Analysis\n",
        "    residuals = y_true - y_pred\n",
        "    logging.info(f\"Residuals:\\n{residuals}\")\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(residuals, label=\"Residuals\", color=\"purple\")\n",
        "    plt.axhline(y=0, color=\"black\", linestyle=\"dashed\")\n",
        "    plt.xlabel(\"Date\")\n",
        "    plt.ylabel(\"Residual Values\")\n",
        "    plt.title(\"Residual Analysis\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Error Distribution Plot\n",
        "    sns.histplot(residuals, bins=30, kde=True, color=\"orange\")\n",
        "    plt.xlabel(\"Error (Actual - Predicted)\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.title(\"Error Distribution\")\n",
        "    plt.show()\n",
        "\n",
        "    # Calculate Error Metrics\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "\n",
        "    # Log Error Metrics\n",
        "    logging.info(f\"MAE: {mae:.4f}\")\n",
        "    logging.info(f\"MSE: {mse:.4f}\")\n",
        "    logging.info(f\"RMSE: {rmse:.4f}\")\n",
        "    logging.info(f\"MAPE: {mape:.2f}%\")\n",
        "\n",
        "    end_time = time.time()\n",
        "    logging.info(f\"Finished the main function in {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeTkDb76KVcS"
      },
      "source": [
        "# 1)Load Dataset\n",
        "Load the dataset using the fetch_ucirepo function and extract features and targets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HN2_MJw1KbIb"
      },
      "outputs": [],
      "source": [
        "# Load the dataset using the fetch_ucirepo function\n",
        "dataset = fetch_ucirepo(id=235)\n",
        "\n",
        "# Extract features and targets\n",
        "X = dataset.data.features\n",
        "y = dataset.data.targets\n",
        "\n",
        "# Combine data into a single DataFrame\n",
        "df = pd.concat([X, y], axis=1)\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jed8gS5kLDU6"
      },
      "source": [
        "# 2)Combine and Preprocess Data\n",
        "Combine the features and targets into a single DataFrame, create a date column, and preprocess the data by filling missing values and converting data types."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04Nf3-gDLGdu"
      },
      "outputs": [],
      "source": [
        "# Create a date column by combining 'Date' and 'Time' columns\n",
        "df[\"date\"] = pd.to_datetime(df[\"Date\"] + \" \" + df[\"Time\"], dayfirst=True)\n",
        "\n",
        "# Drop the original 'Date' and 'Time' columns as they are no longer needed\n",
        "df.drop(columns=[\"Date\", \"Time\"], inplace=True)\n",
        "\n",
        "# Set the 'date' column as the index of the DataFrame\n",
        "df.set_index(\"date\", inplace=True)\n",
        "\n",
        "# Fill missing values using forward fill and backward fill methods\n",
        "df.fillna(method=\"ffill\", inplace=True)\n",
        "df.fillna(method=\"bfill\", inplace=True)\n",
        "\n",
        "# Convert 'Global_active_power' column to numeric, coercing errors to NaN\n",
        "df[\"Global_active_power\"] = pd.to_numeric(df[\"Global_active_power\"], errors=\"coerce\")\n",
        "\n",
        "# Display the first few rows of the preprocessed DataFrame\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tve2NTk-LUOr"
      },
      "source": [
        "# 3) Prepare Dataset for Model\n",
        "Prepare the dataset for the XGBoost model by creating a new DataFrame with the necessary columns and adding date features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gphL4dlOLYZx"
      },
      "outputs": [],
      "source": [
        "# Create a new DataFrame with the necessary columns for the XGBoost model\n",
        "df_xgboost = df[[\"Global_active_power\"]].reset_index()\n",
        "\n",
        "# Rename columns to 'ds' (date) and 'y' (target variable)\n",
        "df_xgboost.columns = [\"ds\", \"y\"]\n",
        "\n",
        "# Add date features: hour, day, month, and year\n",
        "df_xgboost[\"hour\"] = df_xgboost[\"ds\"].dt.hour\n",
        "df_xgboost[\"day\"] = df_xgboost[\"ds\"].dt.day\n",
        "df_xgboost[\"month\"] = df_xgboost[\"ds\"].dt.month\n",
        "df_xgboost[\"year\"] = df_xgboost[\"ds\"].dt.year\n",
        "\n",
        "# Clean NaN and infinite values\n",
        "df_xgboost.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "df_xgboost.dropna(inplace=True)\n",
        "\n",
        "# Display the first few rows of the prepared DataFrame\n",
        "df_xgboost.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CH07bhp6LeWQ"
      },
      "source": [
        "# 4)Split Data into Train and Test Sets\n",
        "Split the dataset into training and testing sets, and further split the training set into chunks for parallel processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65lU4B07Lggm"
      },
      "outputs": [],
      "source": [
        "# Determine the size of the training set (80% of the data)\n",
        "train_size = int(len(df_xgboost) * 0.8)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_df = df_xgboost[:train_size]\n",
        "test_df = df_xgboost[train_size:]\n",
        "\n",
        "# Display the number of rows in the training and testing sets\n",
        "print(f\"Training set size: {len(train_df)} rows\")\n",
        "print(f\"Testing set size: {len(test_df)} rows\")\n",
        "\n",
        "# Split the training set into chunks for parallel processing\n",
        "num_chunks = 4\n",
        "train_chunks = np.array_split(train_df, num_chunks)\n",
        "\n",
        "# Display the number of rows in each chunk\n",
        "for i, chunk in enumerate(train_chunks):\n",
        "    print(f\"Chunk {i+1} size: {len(chunk)} rows\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXN2JaQfLmZU"
      },
      "source": [
        "#5) Train Model in Parallel\n",
        "\n",
        "Train the XGBoost model in parallel using the multiprocessing library to speed up training by utilizing multiple CPU cores efficiently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o93VJNBWMD3d"
      },
      "outputs": [],
      "source": [
        "# Train the XGBoost model in parallel using the multiprocessing library\n",
        "logging.info(\"Starting model training with multiprocessing\")\n",
        "\n",
        "# Use Pool to parallelize the model training process\n",
        "with Pool(num_chunks) as pool:\n",
        "    models = pool.map(fit_model, train_chunks)\n",
        "\n",
        "# Display the number of models trained\n",
        "print(f\"Number of models trained: {len(models)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJ_-LJruMIG2"
      },
      "source": [
        "# 6)Make Predictions\n",
        "Make predictions on the test set using the trained models and combine the forecasts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zwl58hlqMK9k"
      },
      "outputs": [],
      "source": [
        "# Make predictions on the test set using the trained models\n",
        "logging.info(\"Making predictions\")\n",
        "forecasts = [model.predict(test_df.drop(columns=[\"ds\", \"y\"])) for model in models]\n",
        "\n",
        "# Combine the forecasts by averaging the predictions from all models\n",
        "forecast = pd.DataFrame({\"ds\": test_df[\"ds\"], \"yhat\": np.mean(forecasts, axis=0)})\n",
        "\n",
        "# Display the first few rows of the forecasted values\n",
        "forecast.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsNvBzZNMQav"
      },
      "source": [
        "# 7)Align Actual and Forecasted Values\n",
        "Align the actual and forecasted values based on common dates and handle any discrepancies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XeqFCeCTMUzA"
      },
      "outputs": [],
      "source": [
        "# Find common dates between the actual and forecasted values\n",
        "common_dates = test_df.set_index(\"ds\").index.intersection(forecast.set_index(\"ds\").index)\n",
        "if len(common_dates) == 0:\n",
        "    logging.error(\"No common dates found between actual and forecasted values. Check the data.\")\n",
        "else:\n",
        "    # Extract the actual and forecasted values for the common dates\n",
        "    y_true = test_df.set_index(\"ds\").loc[common_dates, \"y\"]\n",
        "    y_pred = forecast.set_index(\"ds\").loc[common_dates, \"yhat\"]\n",
        "\n",
        "    logging.info(f\"Before align - y_true: {len(y_true)}, y_pred: {len(y_pred)}\")\n",
        "\n",
        "    # Align the actual and forecasted values\n",
        "    y_true, y_pred = y_true.align(y_pred, join='inner')\n",
        "    y_true.dropna(inplace=True)\n",
        "    y_pred.dropna(inplace=True)\n",
        "\n",
        "    # Handle any discrepancies in the lengths of y_true and y_pred\n",
        "    if len(y_true) != len(y_pred):\n",
        "        y_pred = y_pred.reindex(y_true.index, method='nearest')\n",
        "\n",
        "    logging.info(f\"After align - y_true: {len(y_true)}, y_pred: {len(y_pred)}\")\n",
        "\n",
        "    if len(y_true) != len(y_pred):\n",
        "        logging.error(f\"Final y_true length: {len(y_true)}, y_pred length: {len(y_pred)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwLwvi2HMYTw"
      },
      "source": [
        "# 8)Visualize the Forecast\n",
        "Visualize the forecasted values using Plotly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rW2KxxxMcBS"
      },
      "outputs": [],
      "source": [
        "# Visualize the forecasted values using Plotly\n",
        "fig = px.line(forecast, x=\"ds\", y=\"yhat\", title=\"Electricity Consumption Forecast\")\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApoT-EzLM4I2"
      },
      "source": [
        "# 9)Plot Actual vs Predicted\n",
        "Plot the actual vs predicted values using Matplotlib."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gkRVYTuWM_jx"
      },
      "outputs": [],
      "source": [
        "# Plot the actual vs predicted values using Matplotlib\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(y_true, label=\"Actual Values\", color=\"blue\")\n",
        "plt.plot(y_pred, label=\"Predicted Values\", color=\"red\", linestyle=\"dashed\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Energy Consumption\")\n",
        "plt.title(\"Actual vs. Predicted Energy Consumption\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Td-SeVF9MgM4"
      },
      "source": [
        "# 10)Residuals Analysis\n",
        "Analyze the residuals by plotting them and examining their distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRLX8WeFMx5C"
      },
      "outputs": [],
      "source": [
        "# Calculate residuals (difference between actual and predicted values)\n",
        "residuals = y_true - y_pred\n",
        "\n",
        "# Log residuals\n",
        "logging.info(f\"Residuals:\\n{residuals}\")\n",
        "\n",
        "# Plot residuals over time\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(residuals, label=\"Residuals\", color=\"purple\")\n",
        "plt.axhline(y=0, color=\"black\", linestyle=\"dashed\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Residual Values\")\n",
        "plt.title(\"Residual Analysis\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hRfrvOFNEIR"
      },
      "source": [
        "# 11)Error Distribution Plot\n",
        "Plot the error distribution using Seaborn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1UBXO5cjNNEO"
      },
      "outputs": [],
      "source": [
        "# Plot the distribution of residuals using Seaborn\n",
        "sns.histplot(residuals, bins=30, kde=True, color=\"orange\")\n",
        "plt.xlabel(\"Error (Actual - Predicted)\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Error Distribution\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IC93EyLpNQQ0"
      },
      "source": [
        "#12)Calculate and Log Error Metrics\n",
        "Calculate error metrics such as MAE, MSE, RMSE, and MAPE, and log them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7XKiwIQNUvz"
      },
      "outputs": [],
      "source": [
        "# Calculate and Log Error Metrics\n",
        "\n",
        "# Calculate Mean Absolute Error (MAE)\n",
        "mae = mean_absolute_error(y_true, y_pred)\n",
        "\n",
        "# Calculate Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_true, y_pred)\n",
        "\n",
        "# Calculate Root Mean Squared Error (RMSE)\n",
        "rmse = np.sqrt(mse)\n",
        "\n",
        "# Calculate Mean Absolute Percentage Error (MAPE)\n",
        "mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "\n",
        "# Log the calculated error metrics\n",
        "logging.info(f\"MAE: {mae:.4f}\")\n",
        "logging.info(f\"MSE: {mse:.4f}\")\n",
        "logging.info(f\"RMSE: {rmse:.4f}\")\n",
        "logging.info(f\"MAPE: {mape:.2f}%\")\n",
        "\n",
        "# Display the error metrics\n",
        "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
        "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
        "print(f\"Mean Absolute Percentage Error (MAPE): {mape:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jslQwUWzNZHq"
      },
      "source": [
        "#13)Start the main function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWSau8-7NdRN"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}